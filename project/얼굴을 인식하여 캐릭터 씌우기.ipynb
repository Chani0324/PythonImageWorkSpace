{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 얼굴을 인식하여 캐릭터 씌우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection vs Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def overlay(image, x, y, w, h, overlay_image): # 대상 이미지(3채널), x, y 좌표,  width, height, 덮어씌울 이미지 (4채널)\n",
    "  alpha = overlay_image[:, :, 3] # BGRA -> 같은 x, y좌표값에 B, G, R, A 4개의 값이 저장되어 있음. 그중 alpha값\n",
    "  mask_image = alpha / 255 # 0 ~255 -> 0 ~ 1 사이의 값 (1: 불투명, 0: 투명)\n",
    "\n",
    "  for c in range(0, 3): # channel BGR\n",
    "    image[y-h:y+h, x-w:x+w, c] = (overlay_image[:, :, c] * mask_image) + (image[y-h:y+h, x-w:x+w, c] * (1 - mask_image)) # alpha값의 투명도를 곱한 4채널의 BGR값 + 대상이미지의 3채널 BGR값에 (1-alpha)값을 곱한 값 -> 결과적으로 불투명한 이미지가 되지만 왜 alpha값을 쓰냐?\n",
    "    # -> 추가적으로 넣을 이미지에 alpha값이 0인 배경 부분을 제거하고 필요한 부분만 넣고 원본 이미지(동영상)를 실행 시키기 위해 alpha값을 사용하는듯?\n",
    "\n",
    "# 얼굴을 찾고, 찾은 얼굴에 표시를 해주기 위한 변수 정의\n",
    "mp_face_detection = mp.solutions.face_detection # 얼굴 검출을 위한 face_detection 모듈 사용\n",
    "mp_drawing = mp.solutions.drawing_utils # 얼굴 특징을 그리기 위한 drawing_utils 모듈 사용\n",
    "\n",
    "# For video input:\n",
    "cap = cv2.VideoCapture('face.mp4')\n",
    "\n",
    "# 이미지 불러오기\n",
    "image_right_eye = cv2.imread('right_eye.png', cv2.IMREAD_UNCHANGED) # 100 x 100, alpha값까지 추가된 정보 불러옴.\n",
    "image_left_eye = cv2.imread('left_eye.png', cv2.IMREAD_UNCHANGED) # 100 x 100, alpha값까지 추가된 정보 불러옴.\n",
    "image_nose = cv2.imread('nose.png', cv2.IMREAD_UNCHANGED) # 300 x 100, alpha값까지 추가된 정보 불러옴.\n",
    "\n",
    "\n",
    "with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection: # model_selection : 0일경우 2m이내, 1일 경우 2m 밖 / min_detection_confidence : 어느 정도의 신뢰도로 얼굴을 인식할건지\n",
    "  while cap.isOpened():\n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        break\n",
    "\n",
    "      # To improve performance, optionally mark the image as not writeable to\n",
    "      # pass by reference.\n",
    "      image.flags.writeable = False\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      results = face_detection.process(image)\n",
    "\n",
    "      # Draw the face detection annotations on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "      if results.detections:\n",
    "        # 6개의 특징 : 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀(귀 구슬점)\n",
    "        for detection in results.detections:\n",
    "          # mp_drawing.draw_detection(image, detection)\n",
    "          # print(detection)\n",
    "\n",
    "          # 특정 위치 가져오기\n",
    "          keypoints = detection.location_data.relative_keypoints\n",
    "          right_eye = keypoints[0]\n",
    "          left_eye = keypoints[1]\n",
    "          nose_tip = keypoints[2]\n",
    "\n",
    "          h, w, _ = image.shape # height, width, channel\n",
    "          right_eye = (int(right_eye.x * w) - 20, int(right_eye.y * h) - 100)\n",
    "          left_eye = (int(left_eye.x * w)+ 20, int(left_eye.y * h) - 100)\n",
    "          nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))\n",
    "\n",
    "          # 양 눈, 코에 동그라미 그리기\n",
    "          # cv2.circle(image, right_eye, 50, (255, 0, 0), 10, cv2.LINE_AA)\n",
    "          # cv2.circle(image, left_eye, 50, (0, 255, 0), 10, cv2.LINE_AA)\n",
    "          # cv2.circle(image, nose_tip, 50, (0, 255, 255), 10, cv2.LINE_AA)\n",
    "\n",
    "          # 이미지 넣기\n",
    "          \n",
    "          # image[(right_eye[1] - int(image_right_eye.shape[0] / 2)) : (right_eye[1] + int(image_right_eye.shape[0] / 2)), (right_eye[0] - int(image_right_eye.shape[1] / 2)) : (right_eye[0] + int(image_right_eye.shape[1] / 2))] = image_right_eye\n",
    "          # image[(left_eye[1] - int(image_left_eye.shape[0] / 2)) : (left_eye[1] + int(image_left_eye.shape[0] / 2)), (left_eye[0] - int(image_left_eye.shape[1] / 2)) : (left_eye[0] + int(image_left_eye.shape[1] / 2))] = image_left_eye\n",
    "          # image[(nose_tip[1] - int(image_nose.shape[0] / 2)) : (nose_tip[1] + int(image_nose.shape[0] / 2)), (nose_tip[0] - int(image_nose.shape[1] / 2)) : (nose_tip[0] + int(image_nose.shape[1] / 2))] = image_nose\n",
    "\n",
    "          # image, x, y, w, h, overlay_image\n",
    "          overlay(image, *right_eye, int(image_right_eye.shape[1] / 2), int(image_right_eye.shape[0] / 2), image_right_eye)\n",
    "          overlay(image, *left_eye, int(image_left_eye.shape[1] / 2 ), int(image_left_eye.shape[0] / 2), image_left_eye)\n",
    "          overlay(image, *nose_tip, int(image_nose.shape[1] / 2), int(image_nose.shape[0] / 2), image_nose)\n",
    "\n",
    "      # Flip the image horizontally for a selfie-view display.\n",
    "      cv2.imshow('MediaPipe Face Detection', cv2.resize(image, None, fx=0.5, fy=0.5))\n",
    "\n",
    "      if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26,  26,  24, ...,  97,  97,  98],\n",
       "       [ 26,  26,  24, ...,  97,  97,  98],\n",
       "       [ 26,  26,  24, ...,  97,  97,  98],\n",
       "       ...,\n",
       "       [139, 139, 138, ..., 129, 129, 129],\n",
       "       [139, 138, 139, ..., 128, 129, 129],\n",
       "       [139, 139, 139, ..., 128, 128, 129]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_right_eye = cv2.imread('right_eye.png', cv2.IMREAD_UNCHANGED)\n",
    "image_right_eye[:, :, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자세한 공부\n",
    "> python opencv readthedocs 검색"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
